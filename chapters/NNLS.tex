%% ========================================================================
%%							NNLS
%% ========================================================================


\chapter{Non-negative least squares (NNLS)}
\label{cha:NNLS}

Another way to find a grouped portfolio that minimizes the deviation defined in formula (\ref{eq:objective_function}) is to solve the problem using mathematical optimization methods. The goal is to find a subset of the portfolio and scale it in a way so that the square deviation becomes minimal. Of course, this approach must ensure that scaling is only possible in the positive direction. This means that it makes no sense to have a negative policy in the grouped portfolio, because it cannot be defined, not to mention explained logically. 

\begin{definition}[Non-negative least squares]\label{def:NNLS}
	Let $P \subset \V$ be a portfolio, \linebreak $A \in \R^{m \times n}$ the matrix with the corresponding cash flows and $b \in \R^m$ the vector with the summed cash flows. Then $x \in \R^n$, the vector of scaling, should be optimized such that:  
	\begin{equation}\label{eq:NNLS}
		\begin{aligned}
			\argmin_x \lVert Ax -b \rVert_2^2 \\
			\text{subject to } x \geq 0
		\end{aligned}
	\end{equation}
\end{definition}

\begin{remark}
	\leavevmode % needed for items to start in new line after remark.
	\makeatletter
	\@nobreaktrue
	\makeatother
	\begin{itemize}
		\item 	The entries of the vector $x$ are the so-called scaling values for the cash flows in matrix $A$. Each entry $x_i, i \in \{1,...,n\}$ that is greater than zero scales to the $i$-th column of matrix $A$ where column $i$ represents the cash flows of the $i$-th policy of the portfolio.
		\item 	It should be noted that this approach optimizes cash flows, not policies. Since there is a one-to-one relationship between cash flows and policies, the scaling factors cannot only be used to scale the cash flows but also to scale the policies in order to create a grouped portfolio. 
		\item 	Scaling policies can also involve risks, as a policy that is scaled by a factor of 2 will not necessarily produce cash flows that are also increased by a factor of 2. This can be attributed to the fact that, for example, non-linearities occur due to discount effects for higher premiums in the tariff.
	\end{itemize}
\end{remark}

\begin{remark}
	The number of policies in the grouped portfolio corresponds exactly to the number of entries greater than zero in the vector $x$.
\end{remark}

One of the well known algorithms for solving the non-negative least squares problem is that of Lawson and Hanson, which uses an active set method. The steps necessary for solving that problem are given in \cite{lawson}. Additionally to those parameters defined in definition \ref{def:NNLS} one also needs a real value variable $\epsilon$ as a stopping criterion.

\begin{algorithm}
	\caption{Non-negative least squares \cite{lawson}}\label{alg:NNLS}
	\begin{algorithmic}
		\\
		\begin{enumerate}
			\item Set $P = \emptyset$, $R = \{1, ..., n\}$, $x$ = $0_{n \times 1}$
			\item Compute $w = A^\top(b - Ax)$.
			\item While $R \neq \emptyset$ and $max(w) > \epsilon$
			\begin{enumerate}[label=\emph{\alph*})]
				\item Find index $j \in R$ such that $w_j = max\{w_t, t \in R\}$.
				\item Move the index $j$ from $R$ to $P$.
				\item Let $A^P$ be $A$ restricted to the variables included in $P$.
				\item Let $s$ be a vector of same length as $x$. Let $s^P$ denote the sub-vector with indexes from $P$, and let $s^R$ denote the sub-vector with indexes from $R$.
				\item Compute $s^P = ((A^P)^\top A^P)^{-1} (A^P)^\top b$
				\item Set $s^R = 0$.
				\item While $min(s^P) \leq 0$
				\begin{enumerate}
					\item Set $\alpha_k = min\frac{x_i}{x_i - s_i}$ for $i$ in $P$ where $s_i \leq 0$
					\item Set $x = x + \alpha_k(s-x)$
					\item Move from $P$ to $R$ all indices $k \in P$ for which $x_k = 0$.
					\item Compute $s^P = ((A^P)^\top A^P)^{-1} (A^P)^\top b$
					\item Set $s^R = 0$
				\end{enumerate}
				\item Set $x$ to $s$
				\item Compute $w = A^\top(b - Ax)$.
			\end{enumerate}
		\end{enumerate}
	\end{algorithmic}
\end{algorithm}

Algorithm \ref{alg:NNLS} consists, apart from the initialization, of a main loop and an inner loop. The loops are highlighted by indentations and start at step 3 and step g) respectively. If for a variable reference is made to those indices of the variable which are contained in the set $R$, then these entries are 0. All indexes that exist in the set P, by contrast, have non-zero values. If such a variable has a negative value, the algorithm either moves it to the positive value range or sets it to zero. By setting a variable to zero, the index is also shifted from the set $P$ to the set $R$. This ensures that the following condition is met at the end of the algorithm.   
\begin{align}\label{equ:final_conditions}
\begin{split}
	x_j &> 0, \quad j \in P \\
	x_j &= 0, \quad j \in R
\end{split}
\end{align}



\begin{remark}\label{rem:gradient}
	Let $f(x) = \lVert Ax -b \rVert_2^2$, then the gradient of $f(x)$ is given by: 
	\begin{equation*}
		\nabla f(x) = \nabla \lVert Ax -b \rVert_2^2 = A^\top(Ax-b)
	\end{equation*}
	\begin{proof}	
		\begin{align*}
			\nabla \lVert Ax -b \rVert_2^2	&= \nabla (Ax-b)^\top (Ax-b) \\
											&= \nabla (x^\top A^\top - b^\top)(Ax-b) \\
											&= \nabla (x^\top A^\top Ax - x^\top A^\top b - b^\top Ax + b^\top b) \\
											&= \nabla (x^\top A^\top Ax - 2x^\top A^\top b + b^\top b) \\
											&= 2 (A^\top A x - A^\top b) \\
											&= 2A^\top (Ax - b)
		\end{align*}
	\end{proof}
\end{remark}

As shown in remark \ref{rem:gradient}, step 2 of algorithm \ref{alg:NNLS} calculates the negative gradient of the ordinary least squares problem. In the next step it is checked whether the inner loop still has to be executed or not. If the index set $R$ correspond to the empty set then all indexes are already in $P$ which means that all entries of $x$ are positive (see formula (\ref{equ:final_conditions})). This case is not desirable, as no compression can be achieved. If $max(w) \leq \epsilon$ is satisfied, the gradient has no entry large enough so that an substantial improvement can be achieved and the algorithm has reached the optimum. If one of the two conditions in step 3) is fulfilled, the main loop is executed. 

The main loop starts with searching for the index of the gradient that is not yet present in the set $P$ and has the highest value. After this index has been moved from the set $R$ to the set $P$, the solution of a restricted least squares problem is calculated in step e). This least squares problem is limited to the columns of the matrix $A$ whose indices occur in the set $P$. The result is then a vector of dimension $P$ which is indicated by the notation $s^P$. To get a solution vector of the dimension $n$, the $n - \vert P \vert = \vert R \vert$ entries $s^R$ of the vector $s$ are filled with zeros - see step f). 

\begin{example}
	Based on algorithm \ref{alg:NNLS}, be $n = 15$, $P = \{2, 3, 4, 5, 7, 9\}$ and  $R = P^c = \{1, 6, 8, 10, 11, 12, 13, 14, 15\}$ then
	\begin{equation*}
		\begin{aligned}[c]	
		s = 
		\left( 
		\begin{array}{c}
		s_{1} \\
		s_{2} \\
		\vdots\\
		s_{15}\\
		\end{array}
		\right)	
		\end{aligned}
		\qquad
		\begin{aligned}[c]
		s^P = 
		\left( 
		\begin{array}{c}
		s_{2} \\
		s_{3} \\
		s_{4} \\
		s_{5} \\
		s_{7} \\
		s_{9} \\
		\end{array}
		\right)	
		\end{aligned}
		\qquad
		\begin{aligned}[c]
		s^R = 
		\left( 
		\begin{array}{c}
		s_{1} \\
		s_{6} \\
		s_{8} \\
		s_{10} \\
		\vdots \\
		s_{15} \\
		\end{array}
		\right)	
		\end{aligned}	
	\end{equation*}
\end{example}

If all components of this least squares solution (i.e. $s^P$) are positive a new solution has been found and the solution vector $x$ is overwritten with $s$ as seen in step h). In step i) a recalculation of the gradient with the solution $x$, adapted in the previous step, is carried out and the main loop is restarted by checking the conditions in step 3. 

If there are non positive entries in the solution of the restricted least squares problem computed in step e), the inner loop is executed. Basically, negative entries in the result vector $s^P$ would lead to a new solution vector $x$ which would also have negative entries without further adjustments, resulting in an undesired solution since $x$ must be non-negative (i.e $x \geq 0$). Therefore, based on the current solution vector $x$ which has only positive entries, a shift is carried out by using $s^P$. In the first step the index $k \in P$ is determined which leads to the smallest scaling factor $\alpha_k$. The current solution vector $x$ is then shifted by using this factor which causes the entry $x_k$ to be zero after the shift. The index $k$ can thus be moved from set $P$ back to set $R$ because the corresponding entry $x_k$ is zero after the shift. In the unlikely event that several entries of the new solution vector $x$ were changed to zero as a result of the shift, all these indexes must of course be moved from set $P$ to set $R$ (see iii.). Based on the new set $P$, a new limited least squares problem will be solved and the inner loop will be redone if necessary.

\begin{remark}
	Since $x_i \geq 0$ at all times and $s_j \leq 0$ within the inner loop, the scaling factor is bounded by $0 \leq \alpha_k \leq 1$.  
\end{remark}

\begin{remark}
	Within the inner loop, at least one index $k$ per iteration is transferred from the set $P$ to the set $R$.  As a result, with each pass of the inner loop, the number of entries in the solution vector $x$ that are not equal to zero is reduced by at least one. Since the cardinality of the set P is finite, it is also determined how often the inner loop is run through at the most, namely $|P| - 1$ times.
\end{remark}

As stated in \cite[p.~163]{lawson}, for many examples the steps of the outer loop are simply repeated by adding another positive coefficient to the solution vector $x$ until one of the termination criteria in step 3 are fulfilled. This observation may be correct for many different application areas, but must be verified with respect to the policy data being optimized. For this purpose, a standard portfolio of policies is used in which the development of the vector $x$ is analysed. The test portfolio consists of 20.000 randomly selected policies using 351 cash flows for each policy. 

Figure (\ref{fig:iterations}) shows how the number of non-negative entries of the solution vector $x$ evolves. After the algorithm is started with an initialization, the solution vector $x$ only has entries that are zero and the next step is to execute the main loop for the first time. So the graph starts in (0,0). After passing through the main loop for the first time, the first entry of $x$ became a positive value and the graph displays this as the point (1,1). Also the next 9 passes of the main loop run exactly as just described so that after 10 iterations 10 entries of $x$ are not equal to zero and the graph is at (10,10). In the next pass of the main loop, the solution $s^P$, see step e), of the restricted least squares problem becomes for the first time not positive for at least one entry and the inner loop is executed. When entering the inner loop, $s^P$ has the dimension 11. Within the inner loop, as explained above, a shift is performed and the index $k$ is shifted from the set $P$ to the set $R$. Due to this shift of the index and the subsequent recalculation of the now dimensionally reduced least squares problem (see iv.), $s^P$ now has only dimension 10. Since none of these 10 entries is not positive, the inner loop is now left. The solution vector still has only 10 positive entries after the 11-th iteration and therefore the point (11,10) results. The just described phenomenon that the inner loop is left again after one iteration, since all negative entries have been removed, can be observed in figure (\ref{fig:iterations}) more often. This is exactly the case when a horizontal line is present. In the above example, it is also easy to see that this phenomenon of horizontal lines occurs more frequently after about 50 iterations. This can be explained by the fact that at the beginning of an optimization the limited least squares problem which need to be solved has a small dimension and therefore it is less likely to get negative solution values. In the progress of the optimization, the achievable improvements of the objective function become smaller and smaller and also the cardinality of the set $P$ increases. The last feature in figure (\ref{fig:iterations}) that can be observed is that of a descending line. This case occurs for the first time at the transition from iteration 28 to iteration 29. In iteration 29 the solution vector of the limited least squares problem $s^P$ has entries that are not positive such as in the case described previously. The difference to before is that the inner loop is not left after one iteration. So there are several loop cycles required to adapt all negative entries by shifting. In the particular case of iteration 29, the inner loop is executed exactly twice until all entries of $s^P$ are positive. Since with each pass of the inner loop the number of positive entries of the solution vector $x$ is reduced by one, a descending line results after 2 passes. It is therefore clear that the steepness of the descent is a measure of how often the inner loop has been executed before all entries were positive. 


\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{figures/chapter_NNLS/number_iterations}
	\caption{Development of the non-negative entries of the solution vector $x$ based on the number of iterations.}
	\label{fig:iterations}
\end{figure}

If not the number of non-negative entries is considered but the development of formula \ref{eq:NNLS} for each iteration, a similar picture emerges. Figure (\ref{fig:residuals}) shows the logarithmic sum of the squared deviations between the fitted cash flows $Ax$ and the reference cash flows $b$. The graph is monotonously falling and can be compared in its characteristics with the previous figure (\ref{fig:iterations}). Starting with the initialization, the solution vector $x$ has only zero entries and therefore $\lVert Ax -b \rVert_2^2$ reduces to $\lVert b \rVert_2^2$ which gives the first data point of the graph at (0, 2.13e+20). Especially at the beginning of the optimization, a new non-negative entry in $x$ is added with each iteration, as seen in figure (\ref{fig:iterations}). This is also reflected in the fact that in figure (\ref{fig:residuals}) the deviation falls significantly for the first iteration. From iteration 50 on there is a clear flattening of the curve which indicates that from then on the improvements of the target function are no longer possible to the same extent as at the beginning of the optimization. This is also confirmed by the fact that after 25\% of the iterations 99.9\% of the reduction of the target function has already taken place. The remaining 0.1\% improvement to the final optimized value of formula (\ref{eq:NNLS}) then takes place in the last 75\% of the iterations. Since, as described in more detail in the next section, the inversion of matrices is one of the most time-consuming steps in terms of execution time, it can be useful to cancel an optimization prematurely.  A large part of the computing time could be saved and at the same time only a fraction of the optimization quality would be lost. 

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{figures/chapter_NNLS/residuals}
	\caption{Development of the deviance based on the number of iterations.}
	\label{fig:residuals}
\end{figure}

\section{Numerical Aspects}
As evident in figure (\ref{fig:iterations}) as well as figure (\ref{fig:residuals}), the optimization is stopped after 214 iterations. The reason for this is neither that a sufficiently large gradient is no longer available nor that the set $R$ is empty. So all conditions of the main loop stated in step 3 of algorithm \ref{alg:NNLS} are still fulfilled. 
Rather, a stable state has occurred which does not allow any further improvement of the results. Reaching a stable state during an optimization is one of several reasons for an optimization to stop.

Facing a stable state involves a very special case of index shifts between the sets $R$ and $P$, which must be handled separately by the algorithm. Starting with the calculation of the gradient in step i), the new index $j$ in $R$ is searched which has the highest gradient. This new index is then moved from the set $R$ to the set $P$ and the constrained least squares problem is calculated as specified in step e). Since the vector $s^P$ now has exactly one negative entry, the inner loop must be entered in the next step. It turns out that exactly that index $j$ of $s$ is negative which was added by the shift from $j$ to $P$ in the last main loop. 
Since the corresponding entry $x_j$ is zero and $s$ has only one negative entry, the scaling factor $\alpha$ also has a value of zero. This leads to the fact that the shift specified in step ii. has no effect and the solution vector $x$ remains the same. In step iii. the previously determined index $j$ will then be shifted back from the set $P$ to the set $R$. In the next iteration of the main loop, the index with the largest gradient is found again as the previously moved index $j$. This will lead to what has just been described, resulting in an infinite loop. A case such as this therefore only occurs if the following two conditions occur simultaneously: 

\begin{enumerate}
	\item The solution vector $s^P$ of the constrained least squares problem has one negative entry.
	\item This negative entry is in vector $s$ exactly at the index that was transferred from set $R$ to set $P$ in the current loop pass in step b).
\end{enumerate}


\begin{remark}
	If the stopping reason of the optimization algorithm is the reaching of a stable status, the number of non-negative entries of the solution vector $x$ must be equal for the last two iterations. This means that in figure (\ref{fig:iterations}) the graph has to end with a horizontal line.
\end{remark}

Another reason that can lead to an unplanned termination of the optimization algorithm is related to the solving process of the restricted least squares problem in step e) and iv). In these steps the solution of the constrained least squares problem is calculated which requires the use of an inverse matrix. If the inverse of $((A^P)^\top A^P)^{-1}$ does not exist, an alternative solution has to be found. The most practicable approach is to test whether an inverse exists using the determinant of $(A^P)^\top A^P$. If this is not the case, remove the just added index $j$ from the set $P$ and replace it with a $j'$ which has the second largest gradient. 
Not only the existence of the inverse matrix is important but also its numerical stability. Therefore it should be ensured that with a small change of the matrix $A$, the inverse matrix $A^{-1}$ do not change significantly in order to get stable results. To determine whether the inversion of a matrix $A$ is numerically stable, a new concept must be introduced which characterizes the numerical stability.



\begin{definition}
	Let $V$ and $W$ be normed vector spaces and $f:V \to W$ a linear operator. Then the operator norm is given by: 
	\begin{align}\label{eq:operator_Norm}
		\lVert f \rVert := \sup_{x \in V \setminus \{0\}} \frac{\lVert f(x) \rVert_W}{\lVert x \rVert_V} = \sup_{\lVert x \rVert_{V} = 1} \lVert f(x) \rVert_W
	\end{align}
\end{definition}

\begin{remark}
	Since every real valued matrix $A \in \R^{m \times n}$ corresponds to a linear map from $\R^n$ to $\R^m$, each pair of norms induces an operator norm. In the special case of choosing the Euclidean norm for both vector spaces, formula (\ref{eq:operator_Norm}) simplifies to:
	\begin{align}\label{eq:matrix_Norm}
		\lVert A \rVert_2 := \max_{x \neq 0} \frac{\lVert Ax \rVert_2}{\lVert x \rVert_2} = \max_{\lVert x \rVert_{2} = 1} \lVert Ax \rVert_2
	\end{align}	
	which is a naturally induced matrix norm called spectral norm. 
\end{remark}

The natural matrix norm thus vividly corresponds to the greatest possible stretching factor, which results from the application of the linear mapping (i.e. matrix) to a unit vector.

\begin{remark}
	The naturally induced matrix norm satisfies the three norm axioms:
	\begin{itemize}
		\item $\lVert A \rVert = 0$ iff $A = 0$ \hfill (being definite)
		\item $\lVert \alpha A \rVert = |\alpha| \lVert A \rVert$ \hfill (being absolutely homogeneous)
		\item $\lVert A + B  \rVert \leq \lVert A \rVert + \lVert B \rVert$ \hfill (being sub-additive)
	\end{itemize}
\end{remark}

\begin{remark}
	The naturally induced matrix norm is also sub-multiplicative, which means that
	\begin{align*}
		\lVert AB \rVert \leq \lVert A \rVert \lVert B \rVert
	\end{align*}
	\begin{proof}
		\begin{align*}
		\lVert AB \lVert 	&= \max_{x \ne 0} \frac{\lVert ABx \lVert}{\lVert x \lVert} \\
							&=\max_{Bx \ne 0}\frac{\lVert ABx \lVert }{\lVert x\lVert } \\
							&=\max_{ Bx\ne 0}\frac{\lVert ABx\lVert}{\lVert Bx \lVert} \frac{\lVert Bx \lVert}{\lVert x \lVert} \\
							&\le \max_{y \ne 0} \frac{\lVert Ay \lVert}{\lVert y \lVert} \max_{x \ne 0} \frac{\lVert Bx \lVert}{\lVert x \lVert} \\
							&= \lVert A \lVert \lVert B \lVert
		\end{align*}
	\end{proof}
\end{remark}

\begin{remark}[\cite{stewart1998matrix}]
	The 2-norm has the following properties:
	\begin{enumerate}
		\item $\lVert A \rVert_2$ = $\sigma_{max}(A)$ \hfill largest singular value of $A$
		\item $\lVert A \rVert_2^2$ = $\lambda_{max}(A^T A)$ \hfill largest eigenvalue of $A^T A$
		\item $\lVert A \rVert_2 = \lVert A^T \rVert_2$
	\end{enumerate}
\end{remark}

\begin{definition}
	Let $A \in \R^{m \times n}$ be a matrix, $\lVert . \rVert$ a matrix norm and $A^+$ the generalized inverse of matrix $A$. Then the condition number of $A$ is defined as:	
	\begin{align}\label{eq:condition_number}
	\kappa(A) := \lVert A^+ \rVert \lVert A \rVert
	\end{align}
\end{definition}

\begin{remark}
	For the special case that matrix $A$ is regular, the pseudo inverse can be replaced with the inverse matrix and the condition number becomes:
	\begin{align}
	\kappa(A) = \lVert A^{-1} \rVert \lVert A \rVert
	\end{align}	
\end{remark}

\begin{remark}
	Let $A$ be a non-singular matrix then a lower bound for the condition number is given by: 
	\begin{align*}
	 1 \leq \lVert I \rVert = \lVert A A^{-1} \rVert \leq \lVert A \rVert \lVert A^{-1} \rVert = \kappa(A)
	\end{align*}
\end{remark}

\begin{remark}
	Let $A \in \R^{m \times n}$ be a matrix and $\lVert . \rVert_2$ the induced matrix norm. Then (\ref{eq:condition_number}) simplifies to: 
	\begin{align*}
		\kappa_2(A) = \frac{\sigma_{max}(A)}{\sigma_{min}(A)}
	\end{align*}	
\end{remark}

As already mentioned above, it is necessary to estimate to what extent changes in the input variables have an effect on the output variables. In the case of the calculation of an inverse matrix the following approach can be followed. Starting from a matrix $A$ which should be inverted another matrix $E$ is defined. This matrix $E$ represents small changes of matrix $A$ and should therefore be seen as perturbation of matrix $A$. In order to determine how numerically stable the inversion of a matrix $A$ is, the following term is considered.
\begin{equation}\label{eq:stable}
	\lVert A^{-1} - (A + E)^{-1} \rVert
\end{equation}
The task now is to find a constant $c$, so that for all sufficiently small matrices $E$ it applies that:
\begin{equation}
	\lVert A^{-1} - (A + E)^{-1} \rVert \leq c \lVert E \rVert
\end{equation}
 
\begin{remark}\label{eq:simplify}
	It applies:
	\begin{equation*}
		(A + E)^{-1} = (I + A^{-1}E)^{-1}A^{-1}
	\end{equation*}
\end{remark}

\begin{corollary}[\cite{AnaII}]\label{eq:Neumann}
	Suppose that B is a bounded linear operator on a Banach space X with $\lVert B \rVert < 1$. Then	
	\begin{equation}
		S = \sum_{k=0}^\infty B^k = (I-B)^{-1}
	\end{equation}  
\end{corollary}
It therefore follows by applying remark \ref{eq:simplify} and corollary \ref{eq:Neumann} that:
\begin{align*}
	(A + E)^{-1} 	&= (I + A^{-1}E)^{-1}A^{-1} \\
					&= (I - (A^{-1}E) + (A^{-1}E)^2 - (A^{-1}E)^3 + h.o.t.) A^{-1} \\
					&= A^{-1} - A^{-1} E A^{-1} + h.o.t.
\end{align*}
 For the estimation of the stability the following inequality can be obtained by using the previous results as well as formula (\ref{eq:stable}):
\begin{align*}
	\lVert A^{-1} - (A + E)^{-1} \rVert & = \lVert A^{-1} - (A^{-1} - A^{-1}EA^{-1} + h.o.t.) \rVert \\
		& = \lVert A^{-1}EA^{-1} - h.o.t. \rVert \\
		& \leq \lVert A^{-1} \rVert^2 \lVert E \rVert +  h.o.t.
\end{align*}
The relative error caused by applying a perturbation to matrix $A$ has therefore an upper bound which is given by: 
\begin{equation}\label{eq:upper_bound}
 \frac{\lVert A^{-1} - (A + E)^{-1} \rVert}{\lVert A^{-1}\rVert} \leq \lVert A \rVert \lVert A^{-1} \rVert \frac{\lVert E \rVert}{\lVert A \rVert} + h.o.t.
\end{equation}

\begin{remark}
	If the naturally induced spectral norm is used as the matrix norm, formula (\ref{eq:upper_bound}) simplifies to:
	\begin{align*}
		\frac{\lVert A^{-1} - (A + E)^{-1} \rVert}{\lVert A^{-1}\rVert} & \leq \kappa_2(A) \frac{\lVert E \rVert}{\lVert A \rVert} + h.o.t. \\
		& = \frac{\sigma_{max}(A)}{\sigma_{min}(A)} \frac{\lVert E \rVert}{\lVert A \rVert} + h.o.t.
	\end{align*}
\end{remark}


To reduce the influence of numerical instabilities, a linear least squares problem is generally solved not by inverting the matrix of the normal equations like in step e) and iv) in algorithm (\ref{alg:NNLS}) but by other numerical methods. A very frequently used method which avoids forming $A^TA$ and inverting it is the $QR$ decomposition.  

\section{QR - decomposition}

A $QR$ decomposition describes the decomposition of a matrix into a product of two matrices with special properties. This decomposition exists for every matrix and can be calculated with different algorithms where the best known are:

\begin{itemize}
	\item Gram–Schmidt
	\item Householder transformation
	\item Givens rotations
\end{itemize}

\begin{definition}
	Let $A \in \R^{m \times n}$ with $m \geq n$ be a matrix. The decomposition of $A$ into a product $A = QR$ with an orthogonal matrix $Q \in \R^{m \times m}$ and an upper triangular matrix $R \in \R^{m \times n}$ is called a $QR$-decomposition of $A$. 
\end{definition}

Considering the fact that $m \geq n$ and the matrix R is always quadratic, it often makes sense to partition both $R$ and $Q$ in a way such that the special structure of these matrices can be used advantageously. Since $R$ is an upper triangular matrix, the last $m-n$ rows of matrix $R$ consist only of zeros. Therefore it makes sense to split the matrix $Q$ into two parts $Q_1$ and $Q_2$ which have $n$ and $m-n$ columns respectively. The $QR$ decomposition is reduced by the use of the special characteristics described above to:
\begin{equation}\label{equ:thin_QR}
	\underbrace{A}_{m \times n} =  \underbrace{Q}_{m \times m} \underbrace{\begin{bmatrix} R_1 \\ 0 \end{bmatrix}}_{m \times n} = [\underbrace{Q_1}_{m \times n} \underbrace{Q_2}_{m \times (m-n)} ]  \underbrace{\begin{bmatrix} R_1 \\ 0 \end{bmatrix}}_{m \times n} = Q_1 R_1
\end{equation}
This notation is often referred to as reduced $QR$ decomposition of $A$ (\cite{trefethen1997numerical}).

\begin{remark} The $QR$ - decomposition is unique if $rank(A) = n$ and the diagonal elements of $R_1$ are required to be positive.
\end{remark}

\begin{remark} Let $Q \in R^{n \times n}$ be an orthogonal matrix and $x \in \R^n$ a vector, then the following properties can be derived:
	\begin{itemize}
		\item $Q^\top Q = I$ \hfill ($Q$ orthogonal)
		\item $\lVert Qx \rVert_2^2 = (Qx)^T (Qx) = x^T Q^T Q x = x^T I x = \lVert x \rVert_2^2$ \hfill (length-invariant)

	\end{itemize}
\end{remark}

The $QR$-decomposition of matrix $A$ can now be used to reduce the numerical instabilities that can occur in algorithm \ref{alg:NNLS}. 

\begin{remark}\label{rem:qr_in_NNLS} 
	Let $A^P$ be the restricted matrix from algorithm \ref{alg:NNLS} and $QR$ the corresponding decomposition such that $A^P = QR$. Then steps e) and iv. from algorithm \ref{alg:NNLS} can be written as:
	\begin{equation}\label{equ:qr_in_NNLS}
		\begin{aligned}
			 s^P 	&= ((A^P)^\top A^P)^{-1} (A^P)^\top b \\
			 		&= ((QR)^\top QR)^{-1} (QR)^\top b \\
			 		&= (R^\top Q^\top Q R)^{-1} R^\top Q^\top b \\
			 		&= R^{-1} (R^\top)^{-1} R^\top Q^\top b \\
			 		&= R^{-1} Q^\top b
		\end{aligned}
	\end{equation}
\end{remark}

A multiplication of formula (\ref{equ:qr_in_NNLS}) from the left with $R$ results in a formula which is particularly easy for the calculation of the coefficients $s^P_i$.
\begin{equation}\label{equ:qr_in_NNLS_opt}
	R s^P = Q^\top b
\end{equation}

Since $R$ is an upper triangular matrix, this system of equations can be solved very easily. Under the assumption that $s^P$ has dimension $n \times 1$ and $Q^\top b$ is denoted as $\tilde{b}$, the parameters can be calculated by backward substitution following the rule:
\begin{align*}
	s^P_n 	&= \frac{\tilde{b}}{r_{nn}} \\
	s^P_i	&= \frac{1}{r_{ii}} \left( \tilde{b}_i - \sum_{j = i + 1}^{n} r_{ij} s^P_j\right) \quad \quad i = n-1, ..., 1
\end{align*}

After showing how algorithm \ref{alg:NNLS} benefits by using the $QR$ decomposition, a way how such a decomposition can be calculated will be presented. In the following, the Householder-transformation, one of the most widespread methods, is derived.

\subsection{Householder transformation}  

The aim of the Householder transformation is to transform matrix $A$ into an upper triangular matrix $R$ by iterative multiplications of so-called Householder matrices $H_i$. 
The procedure is schematically shown in the following example: 

\begin{example}\label{ex:householder} 
	Let $A \in \R^{m \times n}$, $m \geq n$ be the matrix for which a $QR$ decomposition is to be performed. Let $H_1$ and $H_2$ be special Householder matrices. Then the $QR$ decomposition is methodically calculated according to the following pattern. 
	\begin{subequations}
		\begin{align*}
		A &= 
		\begin{pmatrix}
		a_{1,1} & a_{1,2} & \cdots & a_{1,n} \\
		a_{2,1} & a_{2,2} & \cdots & a_{2,n} \\
		a_{3,1} & a_{3,2} & \cdots & a_{3,n} \\
		\vdots  & \vdots  & \ddots & \vdots  \\
		a_{m,1} & a_{m,2} & \cdots & a_{m,n} 
		\end{pmatrix} 
		\\
		H_1 A &= 
		\begin{pmatrix}
		a_{1,1} & a_{1,2} & \cdots & a_{1,n} \\
		0		& a_{2,2} & \cdots & a_{2,n} \\
		0		& a_{3,2} & \cdots & a_{3,n} \\
		\vdots  & \vdots  & \ddots & \vdots  \\
		0		& a_{m,2} & \cdots & a_{m,n} 
		\end{pmatrix} 
		&= 
		\begin{pmatrix}
		a_{1,1} & a_{1,2} & \cdots & a_{1,n} \\
		0 		&  		  &		   &  \\
		0 		&  		  &		   &  \\
		0  		&   	  & \text{\huge$A_2$} &   \\
		0		& 		  &  	   & 
		\end{pmatrix}\\
		H_2 H_1 A &= 
		\begin{pmatrix}
		a_{1,1} & a_{1,2} & \cdots & a_{1,n} \\
		0		& a_{2,2} & \cdots & a_{2,n} \\
		0		& 0		  & \cdots & a_{3,n} \\
		\vdots  & \vdots  & \ddots & \vdots  \\
		0		& 0		  & \cdots & a_{m,n} 
		\end{pmatrix}
		&= 
		\begin{pmatrix}
		a_{1,1} & a_{1,2} & \cdots & a_{1,n} \\
		0 		& a_{2,2} &	\cdots & a_{2,n} \\
		0 		& 0		  &		   &  \\
		0  		& \vdots  & \text{\huge$A_3$} &   \\
		0		& 0		  &  	   & 
		\end{pmatrix}
		\end{align*}
	\end{subequations}
\end{example}

As shown in example (\ref{ex:householder}), by applying the Householder matrix $H_1$ to matrix $A$, the first column of Matrix $A$ is transformed to a multiple of the first unit vector. This transformation is implemented by a mirroring which is derived in the following section. After the transformation of the first column only the submatrix $A_2$ of the matrix $H_1A$ is considered. This matrix consists of one row and one column less, but has a decisive advantage. Considering $A_2$ on its own, the first column can be mirrored to a multiple of the first unit vector as before and the same logic can be used iteratively. Altogether this means that the use of Householder matrices $H_i$ iteratively generates an upper triangular matrix.

\begin{remark}
	Since the sub matrices (i.e. $A_2$, $A_3$, ...) that are to be transformed become in each step one row and one column smaller, this is also the case for the Householder matrices $\tilde{H}_i$. In order to preserve the transformations already carried out in the previous steps, the matrices $\tilde{H}_i$ are therefore enlarged in a way such that:
	\begin{equation*}
		H_i := \left[
		\begin{array}{cc}
		I & 0 \\
		0 & \tilde{H}_i
		\end{array}
		\right]
	\end{equation*}
\end{remark}

The task now is to find a matrix $P$ that represents the desired reflection. To achieve this, a step-by-step approach is chosen. In a first step the reflection of a vector at a hyperplane through the origin in Euclidean space is constructed. Once this general case has been derived, it can be used to construct a reflection such that the the first column of matrix $A$ is transformed to a multiple of the first unit vector.


\begin{figure}
	\caption{Mirroring of $\vec{x}$ to $\vec{Px}$ through hyperplane $H$.}
	\centering
	\begin{tikzpicture}
	% draw the projection hyperplane
	\coordinate [label=left:{$H$}] (p1) at (-1,0);
	\coordinate (p2) at (6.5,0);
	
	%\draw[very thin] (p1) -- node[at start] {$H$} (p2);
	\draw[very thin] (p1) -- (p2);
	
	% draw the projection direction
	\coordinate (v1) at (5,1);
	\coordinate (v2) at (5,-1);
	\draw[very thick, ->] (v1) -- node[near end, right] {$\vec{v}$} (v2);

	
	% draw the vector and projected vector
	\coordinate[label=above:0] (x1) at (0,0);
	\coordinate (x2) at (4,2);
	\coordinate (x3) at (4,-2);
	\coordinate (x4) at (4,0);
	
	\draw[very thick, ->] (x1) -- node[very near end, above] {$\vec{x}$} (x2);
	\draw[very thick, ->] (x1) -- node[very near end, below] {$\vec{Px}$} (x3);
	\draw[very thick, dashed, ->] (x2) -- node[near start, right] {$\vec{w}$} (x4);
	
	\begin{scope}[shift={(4,2)}]
	\draw (0,0) -- (207:.75cm) arc (207:270:.75cm);
	\draw (-85:0.4cm) node[left] {$\theta$};
	\end{scope}
	
	\rightangle{4,0}{90}{.5} % user defined function
	\end{tikzpicture}
	\label{fig:householder}
\end{figure}

For the construction of the mirroring matrix $P$ the case shown in figure (\ref{fig:householder}) is considered. Let $\vec{x}$ be a vector in an Euclidean space and $\vec{Px}$ the vector into which $\vec{x}$ is to be transferred by a mirroring. Furthermore, $H$ is the hyperplane at which the reflection should take place. $H$, that mirror-hyperplane which runs through the origin is defined by the normal vector $\vec{v}$, thus a vector which is orthogonal to the hyperplane. The difference between the vector $\vec{x}$ and the hyperplane $H$ is called $\vec{w}$. The angle enclosed by the vectors $\vec{-x}$ and $\vec{w}$ is called $\theta$. The goal is to identify a relation between $\vec{Px}$ and $\vec{x}$. In the sense of better readability and since there can be no misunderstandings in the following, the vector arrows are omitted from now on. The length of $\vec{w}$ is then given by: 
\begin{equation*}
	\lVert w \rVert 
	= \lVert x \rVert \cos(\theta) 
	= \lVert x \rVert \frac{\langle -x, v \rangle}{\lVert -x \rVert  \lVert v \rVert} 
	= \frac{-x^\top v}{\lVert v \rVert}
\end{equation*}
Where in the second step the definition of the dot product was used. The vector $w$ is then characterized by the length and the direction which leads to: 
\begin{equation*}
	w  
	= \frac{-x^\top v}{\lVert v \rVert} \frac{v}{\lVert v \rVert}
	= -v \frac{x^\top v}{v^\top v}
\end{equation*}
By referencing to figure (\ref{fig:householder}), the mirrored vector $Px$ can now be defined as:
\begin{equation*}
	Px   
	= x + 2w
	= x - 2v \frac{x^\top v}{v^\top v}
	= x - 2 \frac{v v^\top x}{v^\top v}
	= \left(I -  2 \frac{v v^\top}{v^\top v} \right)x
\end{equation*}
The matrix constructed, representing the linear mapping described above is called Householder matrix.  Householder matrices are  defined by a normal vector $v$, i.e. a vector that is orthogonal to the mirror hyperplane and are typically denoted by $H$.
\begin{equation}\label{eq:Householder}
H = I - 2 \frac{v v^\top}{v^\top v}
\end{equation}
Where $I$ in equation (\ref{eq:Householder}) is the identity matrix. In case that $v$ is normalized to length one (\ref{eq:Householder}) simplifies to:
\begin{equation}
H = I - 2 v v^\top
\end{equation}
The concept of Householder matrices can now be used to formalize the process described in example \ref{ex:householder}. Let $x$ be a vector which is to be mirrored to a multiple of the first unit vector $e_1$. This means that a vector $v$ is required, so that with the corresponding Householder-Matrix $H_v$ the following linear transformation can be achieved.
\begin{equation*}
	H_{v} x = c e_1
\end{equation*}
The required reflection vector $v$ now results from normalizing the difference vector and is given by:
\begin{equation*}
	v = \frac{x - ce_1}{\lVert x - ce_1\rVert}
\end{equation*}
A basic property that is important for the construction of a $QR$ decomposition is the fact that Householder matrices are orthogonal. 
\begin{remark}
	Let $H$ be a Householder matrix. Then $H$ is symmetric and orthogonal.	
	\begin{equation*}
		H^\top 	= (I - 2 \frac{v v^\top}{v^\top v})^\top 
				= I^\top - \left( 2 \frac{v v^\top}{v^\top v} \right)^\top 
				= I - 2 \frac{\left( v v^\top \right)^\top}{\left( v^\top v \right)^\top} 
				= I - 2 \frac{v v^\top}{v^\top v} = H	
	\end{equation*}	
	\begin{align*}
		H^\top H = H H 	
		& = (I - 2 \frac{v v^\top}{v^\top v}) (I - 2 \frac{v v^\top}{v^\top v}) \\
		& = I^2 - 2I \frac{v v^\top}{v^\top v} - 2I \frac{v v^\top}{v^\top v} + 4 \frac{v v^\top}{v^\top v} \frac{v v^\top}{v^\top v} \\
		& = I - 4 \frac{v v^\top}{v^\top v} + 4 \frac{v v^\top v v^\top}{v^\top v v^\top v} \\
		& = I - 4 \frac{v v^\top}{v^\top v} + 4 \frac{(v^\top v) v v^\top}{(v^\top v)^2} \\
		& = I
	\end{align*}	
\end{remark}


\begin{remark}
	Let $H_1, H_2, ..., H_n$ be Householder matrices as stated in example \ref{ex:householder}. Then by using the properties proofed in the previous remark the $QR$ decomposition is given by:  
	\begin{align*}
							  & H_n H_{n-1} \cdot \ldots \cdot H_1   A = R   \\
		\Leftrightarrow \quad & Q^\top A = R \\
		\Leftrightarrow \quad & QQ^\top A = QR \\
		\Leftrightarrow \quad & A = QR \\
	\end{align*}
\end{remark}

After demonstrating how a QR decomposition can be performed using Householder transformations, the next section is dedicated to the question of performance. 

\subsection{Performance}

Even if runtime analyses of algorithm \ref{alg:NNLS} have shown that a large part of the computing time had to be dedicated to the calculation of the gradients, the importance of finding the solution for the least squares problems must not be underestimated and will therefore be analysed as well. Since not only computing time but also the numerical stability of the grouping process is of great interest in practice, the runtime and accuracy of different approaches for solving the least squares problem are compared in this section.  Obviously, an implementation of a grouping algorithm which is numerically stable on the one hand and fast on the other hand with regard to the computing time is the preferred solution. Therefore seven different implementations will be compared using only functions that are available in \textsf{R} by default. Since these are standard functions in the area of linear algebra, it can be assumed that the functions used are already highly optimized with respect to performance. In particular, some functions access implementations of the software packages LINPACK and LAPACK directly and therefore act only as wrappers. Examples of such \textsf{R}-functions are \texttt{solve} and \texttt{qr} whose default methods are interfaces to the LAPACK routines \texttt{DGESV} and \texttt{ZGESV} as well as \texttt{DQRDC} from the LINPACK package. 

\begin{itemize}
	\item \texttt{DGESV} computes the solution to a real system of linear equations $A * X = B$, where $A$ is an N-by-N matrix and $X$ and $B$ are N-by-NRHS matrices \cite{lapack}.
	\item \texttt{DQRDC} uses householder transformations to compute the qr factorization of an n by p matrix x.  column pivoting based on the 2-norms of the reduced columns may be performed at the users option \cite{linpack}.
\end{itemize}

The two software libraries LINPACK and LAPACK are written in Fortran and enjoy great popularity in many areas of application due to their efficient implementations with respect to memory usage and computational speed. Of course, there is also a big number of packages available on CRAN (Comprehensive R Archive Network) that offer different implementations for solving least squares problems. Since a comprehensive analysis of these implementations is not possible due to the daily growing number of packages available, the focus is on standard functions available in \textsf{R}. The basic \textsf{R}-functions used to perform the comparison are therefore \texttt{solve}, \texttt{backsolve}, \texttt{qr.solve}, \texttt{qr} and \texttt{t}. The notation used in algorithm \ref{alg:NNLS} steps e) and iv. is simplified for the following comparison to the degree that the restriction to set $P$ is not explicitly specified, i.e. $A = A^P$. The first three approaches pursue a solution without the calculation of a $QR$ decomposition whereas the last four approaches are based on a $QR$ decomposition. 


\begin{enumerate}[label={\bfseries Method \arabic*:}, leftmargin=*, labelindent=1em, series = quest]
	\item This method uses the calculation procedure for solving a least squares problem given in algorithm \ref{alg:NNLS}. By using the function \texttt{solve} with only one matrix as parameter the inverse of that matrix will be returned, i.e. \texttt{solve(A) $\estimates Ax = I \Leftrightarrow x = A^{-1}$}. 
	\begin{align*}
		s = (A^\top A)^{-1} A^\top b
	\end{align*}
\end{enumerate}

\begin{enumerate}[series = inform]
	\item[] \begin{lstlisting}[otherkeywords={\%*\%}, numbers=none]
			solve((t(A) %*% A)) %*% t(A) %*% b
			\end{lstlisting}
\end{enumerate}


\begin{enumerate}[resume*=quest]
	\item For this method, the initial approach from the previous method is transformed in such a way that no inverse has to be calculated any more. The least squares problem is then given by:  
	\begin{align*}
		(A^\top A)s =  A^\top b
	\end{align*}
\end{enumerate}
\begin{enumerate}[resume*=inform]
	\item[] \begin{lstlisting}[otherkeywords={\%*\%}, numbers=none]
			solve(t(A) %*% A, t(A) %*% b)
			\end{lstlisting}
\end{enumerate}


\begin{enumerate}[resume*=quest]
	\item The last approach using no $QR$ decomposition solves the least squares problem using the \texttt{crossprod} function. According to the documentation \texttt{crossprod} should be slightly faster than a direct calculation via the transposed matrix. The approach is therefore identical to the one from method 2 except that a different implementation is used:
	\begin{align*}
		(A^\top A)s =  A^\top b
	\end{align*}
\end{enumerate}
\begin{enumerate}[resume*=inform]
	\item[] \begin{lstlisting}[otherkeywords={\%*\%, qr.R}, numbers=none]
	solve(crossprod(A), crossprod(A,b)))
	\end{lstlisting}
\end{enumerate}


\begin{enumerate}[resume*=quest]
	\item This approach uses a $QR$ decomposition and directly implements formula (\ref{equ:qr_in_NNLS}) derived in remark \ref{rem:qr_in_NNLS} for the solution of the least squares problem:
	\begin{align*}
		s = R^{-1} Q^\top b.
	\end{align*}
\end{enumerate}
\begin{enumerate}[resume*=inform]
	\item[] \begin{lstlisting}[otherkeywords={\%*\%, qr.R}, numbers=none]
			deco <- qr(A, LAPACK = FALSE)
			solve(qr.R(deco)) %*% t(qr.Q(deco)) %*% b
			\end{lstlisting}
\end{enumerate}


\begin{enumerate}[resume*=quest]
	\item This approach again uses a $QR$ decomposition and applies formula (\ref{equ:qr_in_NNLS_opt}), which is a transformation of formula (\ref{equ:qr_in_NNLS}). Therefore no inverse has to be calculated to solve the least squares problem:
	\begin{align*}
		Rs = Q^\top b.
	\end{align*}
\end{enumerate}
\begin{enumerate}[resume*=inform]
	\item[] \begin{lstlisting}[otherkeywords={\%*\%, qr.R}, numbers=none]
			deco <- qr(A, LAPACK = FALSE)
			solve(qr.R(deco), t(qr.Q(deco)) %*% b)
			\end{lstlisting}
\end{enumerate}


\begin{enumerate}[resume*=quest]
	\item This method again uses a $QR$ decomposition, but also makes use of the special form of the decomposition where $R$ is an upper triangular matrix. The function \texttt{backsolve} solves a system of linear equations where the coefficient matrix is upper triangular \cite{R}:
	\begin{align*}
		Rs = Q^\top b
	\end{align*}
\end{enumerate}
\begin{enumerate}[resume*=inform]
	\item[] \begin{lstlisting}[otherkeywords={\%*\%, qr.R}, numbers=none]
			deco <- qr(A, LAPACK = FALSE)
			backsolve(qr.R(deco), t(qr.Q(deco)) %*% b)
			\end{lstlisting}
\end{enumerate}


\begin{enumerate}[resume*=quest]
	\item The last method uses a $QR$ decomposition as well. The result of the $QR$ decomposition is not saved in a temporary variable as before, but is passed directly to the \texttt{qr.solve} method. 	
	\begin{align*}
		QRs = b
	\end{align*}
\end{enumerate}
\begin{enumerate}[resume*=inform]
	\item[] \begin{lstlisting}[otherkeywords={\%*\%, qr.R}, numbers=none]
			qr.solve(qr(A, LAPACK = FALSE), b)
			\end{lstlisting}
\end{enumerate}

In order to determine how the number of columns of matrix $A$ affects the run time, the following setup was chosen. Matrix $A$ was defined as a square matrix with $n = 5000$, where the entries of $A$ are uniformly distributed between $0$ and $100000$, i.e. $A \in \R^{n \times n}$. The values of vector $b$ are uniformly distributed between $0$ and $1000$. By defining matrix $A$ and vector $b$, the result vector $s$ is determined as well. For each of the seven methods, the number of columns used from matrix $A$ was successively increased and then the least squares problem was solved. The results obtained in the test runs were produced with the following configuration: 
\begin{itemize}
	\item Processor: Intel$^{®}$ Core™ i7-6700
	\item RAM: 64GB
	\item \textsf{R}-version: 3.5.0
\end{itemize}
\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{figures/chapter_NNLS/computation_time}
	\caption{Computation time for solving the least squares problem $As = b$ in seconds.}
	\label{fig:comp_time}
\end{figure}
In order to obtain reliable results, all calculations were repeated ten times and the individual results averaged. Figure (\ref{fig:comp_time}) shows how the time needed to solve the least squares problem increases with the number of columns used. It is important to note that due to illustration purposes the methods are presented separately based on their underlying approach, but both panels use the same scaling. The left part of the graph shows the results of methods one to three, i.e. all those methods that do not use a $QR$ decomposition to solve the least squares problem. Even if it is difficult to recognize in the left panel of figure (\ref{fig:comp_time}) due to the scaling, the data shows that method one generally has a significantly longer runtime than method two and three even for small values of $n$. Starting from a column number of about 1000, it is clearly visible that the runtime for method one increases significantly faster than for the other two methods. This can be attributed to the fact that method one requires the explicit calculation of an inverse matrix which is not advisable.  When an inverse is needed rather an $LU$ composition should be performed \cite{lund1978hct}. Moreover, there are no significant differences between method two and method three, although method two generally has a slightly shorter runtime. The usually existing speed advantage given in the help of the \texttt{crossprod} function could not be verified. In the right part of figure (\ref{fig:comp_time}), the results of methods four to seven are shown, i.e. all those methods that use a $QR$ decomposition. In general, it can be seen that, with the exception of method seven, all methods are considerably slower than those without $QR$ decomposition. Method four shows significantly longer running times than all other methods, which is due to the fact that both, a $QR$ decomposition and a matrix inversion must be performed. Methods five and six behave very similarly for the most part, with a small runtime advantage for method six being observed as the number of columns increases. Since the two methods are basically identical and method six uses only the special structure of the upper triangular matrix, it can be assumed that this advantage is only effective for larger systems of equations. The method that distinguishes significantly from the others in terms of runtime is method seven. Over the entire range, its computational time is significantly shorter than that of the other methods. Since method seven is the only method that does not temporarily save the result of the QR decomposition but directly processes it, it seems reasonable to conclude that this already results in a significant performance advantage. Considering the results regardless of whether a $QR$ decomposition was used or not, the following summary results:

\begin{itemize}
	\item Methods two, three and seven are to be classified as equivalent in terms of runtime and are also the fastest altogether.
	\item Methods one, five and six have a similar runtime, with method one always performing best.
	\item The slowest method by far is method four. 
\end{itemize}

After analysing how the runtime of the different approaches behave in relation to the number of columns, there is still a need to analyse their accuracy. As mentioned above, methods two, three and seven scale best with an increasing number of columns of matrix $A$. Since method seven uses a $QR$ decomposition, while methods two and three do not use it, an important aspect is to compare their accuracy. Figure (\ref{fig:avg_errors}) shows how much the calculated solutions $s_{LS}$ deviate on average in absolute values from the actual values $s_{true}$, i.e. $err_{avg} = \sum_{i = 1}^{n} \frac{1}{n}| s_{LS}^i - s_{true}^i | $.
\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{figures/chapter_NNLS/errors_per_col}
	\caption{Comparison of average absolute errors based on method and number of columns used from matrix A}
	\label{fig:avg_errors}
\end{figure}
Each box represents the deviations of all seven methods for a fixed number of columns. For example, the box in the upper right corner of figure (\ref{fig:avg_errors}) shows the results for a matrix $A$ with 300 columns. In contrast to figure (\ref{fig:comp_time}), the data points for the column numbers 10, 50, 100, 500 and 1000 were not shown. This is because by omitting the plots a better readability and arrangement was possible without suffering a loss of information. It should therefore be noted once again that all missing boxes basically provide the same results as those shown here and therefore do not provide any information gain. Considering the nine panels, a clear pattern can be identified. In each individual case  methods one to three show on average higher deviations than methods four to seven. Taking the definitions of the methods into account it can be revealed that methods four to seven are based on a $QR$ decomposition. Within those four methods no significant difference can be seen with respect to the deviations. The results of the simulation therefore suggest that all methods based on a $QR$ decomposition are equivalent in terms of accuracy. Methods one to three, which don't use a $QR$ decomposition, show a differentiated picture. Methods two and three show similar deviations, which is not surprising because of their definition, as they differ only in the functions \texttt{t} and \texttt{crossprod}. For method one, there are strong indications that the average deviation increases with the number of columns. In the case that matrix $A$ has 150 columns, the average differences between $s_{LS}$ and $s_{true}$ are significantly smaller for method one than for method two or method three. If the number of columns is increased step by step, this difference becomes smaller and smaller. Looking at the case where matrix $A$ has 3000 columns, it is evident that method one has the highest deviation of all tested methods. Since method one is the only method which uses the calculation of an inverse, the results leads to the conclusion that calculating an inverse becomes more unstable as the number of columns increases. This shows, as already mentioned in previous sections, that an algorithm for finding the solution of the least squares problem should be implemented, which doesn't rely on inverse matrix calculations. After both the execution time and the accuracy of the seven approaches have been examined, the following picture emerges:

\begin{itemize}
	\item In all test cases considered, it could be proven that those methods that use a $QR$ decomposition show the highest accuracy.
	\item Among those methods that use a $QR$ decomposition, the fastest is the one that does not temporarily store the $QR$ results but directly processes them. 
	\item Looking at the fastest methods of each group ($QR$ decomposition used or not) no significant difference can be found.
	\item If an inverse matrix is calculated, the more columns the matrix has, the worse the overall accuracy gets for the test sample.
	\item The calculation time required to solve the least squares problem scales exponentially with the number of columns.
\end{itemize} 

Taking all aspects into account, method seven is the preferred method in terms of both runtime and accuracy. 