%% ========================================================================
%%							Introduction
%% ========================================================================


\chapter{Introduction}
\label{cha:introduction}

A fundamental problem faced by many insurance companies is the projection of the insurance portfolio into the future. In a projection, the future cash flows for each individual policy must be calculated on the basis of actuarial principles and then saved for further analysis. Depending on the purpose of the forecast, these cash flows have to be provided either on a monthly or annually basis.Together with the contractually agreed benefits to the policyholder, a wide variety of other parameters must also be taken into account and modelled in such projections. These include, for example, all contract changes that may occur during the duration of the contract, such as a premium pause, a surrender or the occurrence of a claim. While the calculation of a small set of policies over a relatively short time horizon does not pose a challenge in terms of computation time, this fact changes greatly when projecting entire portfolios. This problem is particularly severe for life insurance portfolios. These contracts often have durations of several decades and the portfolios tend to grow over time as fewer policyholders leave than new ones are added. Even with the optimistic assumption that the complete projection of a single contract only takes a hundredth of a second, the computing time adds up to several hours for a portfolio with several million policies. As the liabilities are projected over a period of several decades, it is also essential to simulate the development of the assets, underlying the liabilities, over this period. Even under the assumption that the assets can be represented by a small number of different types of investments, their projection additionally increases the runtime of the projection. The simultaneous simulation of the asset and liability portfolios results in further factors that have to be taken into account with respect to the behaviour of policyholders during the contract term. If one assumes that the policyholder behaviour during the projection period also depends on external parameters such as the current interest rates on savings, one also has to include that effect into the simulation. This finally results in a dynamic interaction of all components which can be summarized as follows:

\begin{itemize}
	\item Project all contracts to the next year
	\item Simulate the development of the assets
	\item Determine how to policyholders will behave and whats the effect on the assets
	\item Simulate contracts again
\end{itemize}



This thesis reviews the currently used technique for grouping policies together and introduces furthermore some new approaches on how life insurance policies can be grouped together. We will therefore highlight drawbacks and advantages with a special emphasis on the regulatory requirements of every approach discussed. Theoretical considerations as well as practical implementations and tests with real world data will provide us some information on which method an insurance company should work with in order to obtain the best grouping results.  

This thesis is structured as follows: First we give an overview on the legal framework which lays down the minimum requirements for grouping-approaches in insurance companies and introduce the two types of statistical learning, namely supervised and unsupervised. We then discuss how sensitive main characteristics of a policy are with respect to the interest rate, the age or the duration to get a better understanding on which parameters are important for grouping purposes. We therefore make a sensitivity analysis with real world data and a widely used projection tool . In the next chapter we introduce the currently used unsupervised learning algorithm k-means and derive some theoretical findings. 
\todo{Weitere Details f√ºr jedes Kapitel folgen.}


%--------------------------------------------------------------------- Legal Framework -----------------------------------------------------------------------
\section{Legal Framework}

Solvency II - entered into force on 1 January 2016 - is the European framework for a common insurance supervision. It is intended to achieve a harmonization of the European insurance sector and was implemented in accordance with the Lamfalussy architecture which works on a 4 level basis \cite{Lamfalussy_homepage}. The most significant elements and aims of the new regulation framework can be studied on the homepage of the financial market authority (FMA) \cite{FMA_homepage} and on the homepage of the  European Insurance and Occupational Pensions Authority (EIOPA) \cite{EIOPA_homepage}. This work is intended not to cover all aspects and aims of the new Solvency II regulation framework but focuses on the topic of data quality regarding to the actuarial function. In order to meet all the requirements imposed by Solvency II, insurance companies need to process large amounts of data within a short period. One critical aspect of these calculations is the projection horizon which however should cover the full lifetime of all obligations as stated in \cite{Time_horizon}:
\articlequote{3.83.}{}{The projection horizon used in the calculation of best estimate should
cover the full lifetime of all obligations related to existing insurance and
reinsurance contracts on the date of the valuation.}
\articlequote{3.84.}{}{The determination of the lifetime of insurance and reinsurance obligations shall be based on up-to-date and credible information and realistic assumptions about when the existing insurance and reinsurance obligations will be discharged or cancelled or expired.}
Another aspect needed to be considered is the fact that cash flow calculations need to be done for a variety of different economic scenarios which yields to an enormous computational effort. Due to the tight time schedule, insurance companies are looking for new possibilities to speed up these time consuming calculations. One approach is not to make all these calculations on a per policy level, but on a grouped level where similar policies are grouped together and represented by only a few policies. This approach raises the question of how to maintain data quality as mentioned in the level 1 directive \cite{Directive} while reducing the number of policies.

\articlequote{Article 82}{Data quality and application of approximations, including
case-by-case approaches, for technical provisions}{Member States shall ensure that insurance and reinsurance undertakings
have internal processes and procedures in place to ensure the appropriateness, completeness and accuracy of the data used in the calculation of their technical provisions...}

By publishing the level 2 regulations, supplementing the level 1 directive \cite{Directive} the European Commission is getting more specific on data quality (Article 19 in \cite{Regulations}) and also formulates concrete requirements for grouped policies \cite{Regulations}.

\articlequote{Article 35}{Homogeneous risk groups of life insurance obligations}{The cash flow projections used in the calculation of best estimates for life insurance obligations shall be made separately for each policy. Where the separate calculation for each policy would be an undue burden on the insurance or reinsurance undertaking, it may carry out the projection by grouping policies, provided that the grouping complies with all of the following requirements:
\begin{enumerate}[label=\emph{\alph*})]
\item there are no significant differences in the nature and complexity of the risks underlying the policies that belong to the same group;
\item the grouping of policies does not misrepresent the risk underlying the policies and does not misstate their expenses;
\item the grouping of policies is likely to give approximately the same results for the best estimate calculation as a calculation on a per policy basis, in particular in relation to financial guarantees and contractual options included in the policies.
\end{enumerate} }

These level 2 regulations are a reference point on what to consider when grouping policies together and they are even further specified in the level 3 guidelines issued by EIOPA\cite{Guidelines_TP}. Further details on the level 3 guidelines including feedback statements to the consultation paper (EIOPACP-14/036) and the guidelines can be obtained from \cite{Final_Report}.

%--------------------------------------------------------------------- Statistical Learning -----------------------------------------------------------------------

\section{Statistical Learning}

Statistical learning refers to a set of methods which deals with predicting outcomes based on input variables or finding patterns in data sets. In order to accomplish the task of grouping together similar policies, different approaches from statistical learning can be applied. All these methods can be classified either as supervised or unsupervised. Within the framework of supervised methods, statistical models try to predict output variables $y_i$ based on some input variables $x_i$ where the relation $y=f(x)$ is unknown. It is therefore indispensable to have input as well as output data to parameterize such a model in order to find a prediction $\hat f$ of $f$. Unsupervised methods, in contrast, are used when inputs $x_i$ but no corresponding outputs $y_i$ are available. These methods then try to find some hidden patterns within to data. The task of grouping insurance policies involves many different aspects. On the one hand we have all data needed to apply supervised methods, but on the other hand we are only interested in the patterns that can be revealed by an unsupervised method. The input variables are given by the characteristics of each policy and the corresponding output variables are determined by the projection tool. Our primary goal is not to get a good $\hat f$ because the projection tool, which stands for $f$, is already known. We are more interested in hidden patterns that can be used for grouping purposes. In a first step we will apply unsupervised methods to the data and try to group the policies based on their characteristics and their cash flows. In a further step we will try to use the additional information of $f$ to improve the grouping results if possible. 

