%% ========================================================================
%%							Introduction
%% ========================================================================


\chapter{Introduction}
\label{cha:introduction}

A fundamental problem faced by many insurance companies is the projection of the insurance portfolio into the future. In a projection, the future cash flows for each individual policy must be calculated on the basis of actuarial principles and then saved for further analysis. Depending on the purpose of the forecast, these cash flows have to be provided either on a monthly or annually basis.Together with the contractually agreed benefits to the policyholder, a wide variety of other parameters must also be taken into account and modelled in such projections. These include, for example, all contract changes that may occur during the duration of the contract, such as a premium pause, a surrender or the occurrence of a claim. While the calculation of a small set of policies over a relatively short time horizon does not pose a challenge in terms of computation time, this fact changes greatly when projecting entire portfolios. This problem is particularly severe for life insurance portfolios. These contracts often have durations of several decades and the portfolios tend to grow over time as fewer policyholders leave than new ones are added. Even with the optimistic assumption that the complete projection of a single contract only takes a hundredth of a second, the computing time adds up to several hours for a portfolio with several million policies. As the liabilities are projected over a period of several decades, it is also essential to simulate the development of the assets, underlying the liabilities, over this period. Even under the assumption that the assets can be represented by a small number of different types of investments, their projection additionally increases the run time of the projection. The simultaneous simulation of the asset and liability portfolios results in further factors that have to be taken into account with respect to the behaviour of policyholders during the contract term. If one assumes that the policyholder behaviour during the projection period also depends on external parameters such as the current interest rates on savings, one also has to include that effect into the simulation. This finally results in a dynamic interaction of all components which can be summarized in a simplified pseudo code given in algorithm \ref{alg:interaction}:

\begin{algorithm}
	\caption{Simplified dynamic interaction scheme of portfolio projection}\label{alg:interaction}
	\begin{algorithmic}
		\\
		\begin{enumerate}
			\item Initialisation of assets and liabilities at time $t = 0$.
			\item  Iteration till the end of the projection horizon is reached:
			\begin{enumerate}[label=\emph{\alph*})]
				\item Calculation of one time step for the liabilities. This means that all policies are projected to the next year ($t \mapsto t+1$). 
				\item Calculation of one time step for the assets. This means that the returns of all investments are simulated for the next year.
				\item Simulation of the dynamic policyholder behaviour based on the current economic parameters from the previous step.
				\item Application of pre-defined management decisions based on current developments in assets and liabilities. This means that in this step it is determined how high the dividend per share, for example, will be for this year. 
			\end{enumerate}
		\end{enumerate}
	\end{algorithmic}
\end{algorithm}

Note that during an iteration (algorithm \ref{alg:interaction}, 2a - 2d ) all results must be kept in memory until every decision for that loop is made and the next iteration starts. Assuming that there is a portfolio of several million policies and for each policy at least a few dozen variables are relevant for the decision making process, a memory requirement of at least hundreds of million individual values emerges. Also the simulations of the assets side and the subsequent management decisions have a considerable memory requirement. All this leads to the fact that the projection models currently used in the insurance industry are extremely demanding in terms of their memory requirements and execution time. There are at least two different ways to address this problem:

\begin{itemize}
	\item Outsourcing of calculations to an external high performance infrastructure.
	\item Data compression in the sense that similar policies are grouped together and only the grouped portfolio is calculated.
\end{itemize}

With the increasing availability of cloud services, it is already an option these days to outsource complex computing operations to specialized providers. It is possible to rent different types of computing capacities for a certain period of time without much effort and run the projections there. The big advantage of this option is that there is no need to set up, maintain and configure an infrastructure. Since personal data such as age or gender are always used in the projection of insurance contracts, potential problems could arise with respect to the General Data Protection Regulation \cite{datenschutz}. Many companies have therefore decided to compress the insurance portfolio and therefore reducing the memory requirements using various techniques. It is then feasible to calculate the projections with infrastructure that is owned by the company. This approach has the big advantage that it avoids possible problems with data protection issues and also reduces the dependency on external services. On the other hand, an additional effort has to be made to compress the portfolio in a proper way. Of course, it is of utmost importance that the compressed portfolio has the same characteristics and produces the same cash flows as the non-compressed one. Exactly here, this thesis kicks in and shows possible ways, how a portfolio of insurance contracts can be compressed. 

%--------------------------------------------------------------------- Problem formulation -----------------------------------------------------------------------
\section{Problem formulation}
For the sake of simplicity all values calculated by the projection tool are referred to as cash flows regardless of whether they are non-cumulative values such as the reserve or actual cash flows such as the premium. The task is to find a portfolio with a reduced number of policies, the so called grouped portfolio, so that the projected cash flows match those from the reference  portfolio as accurately as possible. In order to be able to define what accurately means in terms of cash flow deviations, some basic concepts must be defined first . 

\begin{definition}
	Let $\V$ = \{V, V is a valid insurance contract\} be the set of all valid insurance contracts. A finite set of elements $P \subset \V$ is called a portfolio and the number of policies within that portfolio is determined by its cardinality.
\end{definition}

\begin{definition}
	Let $P \subset \V$ be a portfolio of n policies (i.e. $\vert P \vert$ = n) and $m$ the number of projected cash flows by the simulation $s$. Then the individual cash flows corresponding to $P$ are encoded in $A \in \R^{m \times n}$ and the summed cash flows are encoded in $b \in \R^m$.

	\begin{equation*}
	\begin{aligned}[c]
	s: \V &\rightarrow R^{m \times n}\\
		P & \mapsto s(P)=A\\
	\end{aligned}
	\qquad \qquad
	\begin{aligned}[c]
	s: \V &\rightarrow R^m\\
		P & \mapsto A \cdot \mathbbm{1}_{n \times 1} = b \\
	\end{aligned}
	\end{equation*}
	
\end{definition}

\begin{remark}
	It should be noted that the definition of portfolio is ambiguous. It can include anything from a single policy to all policies held by an insurance company. The usual segmentation into portfolios is often based on the following characteristics:
	\begin{itemize}
		\item All contracts of a singe tariff are put into one portfolio.
		\item All contracts of a tariff group (endowment insurances, pension insurances,...) are put into one portfolio. 
		\item All contracts sold in the same country are put into one portfolio.
	\end{itemize} 
\end{remark}

\begin{example}
	Let $P \subset \V$ be a portfolio with $\vert P \vert$ = 1000, the number of cash flows $m = 180$. Then the cash flows generated by the projection software can be described as:
	
	\begin{equation*}
	\begin{aligned}[c]	
		A= 
		\left( 
			\begin{array}{cccc}
				a_{1,1} 	& a_{1,2} 	& \cdots 	& a_{1,1000} \\
				a_{2,1} 	& \ddots	&  			& a_{2,1000} \\
				\vdots 		&  			& \ddots 	& \vdots \\
				a_{180,1} 	& a_{180,2}	& \cdots 	& a_{180,1000} \\
			\end{array}
		\right)	
	\end{aligned}
	\qquad
	\begin{aligned}[c]
		b = 
		\left( 
			\begin{array}{c}
			b_{1} \\
			b_{2}\\
			\vdots\\
			b_{180}\\
			\end{array}
		\right)	
		=
		\left( 
			\begin{array}{c}
			\sum_{i = 1}^{1000} a_{1,i} \\
			\sum_{i = 1}^{1000} a_{2,i}\\
			\vdots\\
			\sum_{i = 1}^{1000} a_{180,i}\\
			\end{array}
		\right)	
	\end{aligned}	
	\end{equation*}
\end{example}

A column in matrix $A$ describes all cash flows generated by this policy. One row of matrix $A$ describes the same cash flow generated by the different policies. By calculating line totals, the corresponding cash flow is obtained at portfolio level. The individual policy by policy cash flows are therefore given in matrix $A$, and the portfolio cash flows in vector $b$. 

\begin{definition}
	Let $P \subset \V$ be a portfolio with $\vert P \vert = n$, $\tilde{P} \subset \V$  the grouped portfolio with $\vert \tilde{P} \vert = \tilde{n}$, $\tilde{n} < n$ and $w \in \R^m_{\geq 0} = \{x \in \R^m, x \geq 0\}$ a vector of weights. Then the weighted sum of squares of the cash flows over the entire projection horizon between the grouped portfolio $\tilde{P}$ and the ungrouped one $P$ is given by: 
	
	\begin{equation}\label{eq:objective_function}
		WSS_{total} = \lVert w \circ (b - \tilde{b})\lVert_2^2
	\end{equation}
\end{definition}

The weighting parameter $w$ makes it possible to make deviations from certain cash flows more or less important by weighting them differently. This can be useful, for example, to weight deviations at the end of the projection period less heavily compared to deviations at the beginning of the projection horizon. Particularly with projection horizons of 30 years and more, it may make sense to weight deviations in cash flows less strongly at a later point in time. 

\begin{remark}
	In a situation where every deviation of cash flows between the grouped and the ungrouped portfolio is equally important, the elements of $w$ are all one. In this situation equation (\ref{eq:objective_function}) simplifies to:	
	\begin{equation}\label{eq:objective_function_simple}
		WSS_{total} = \lVert b - \tilde{b}\lVert_2^2
	\end{equation}	
\end{remark}

The aim is to develop a method which is capable of finding a suitable grouped portfolio for each portfolio at hand so that (\ref{eq:objective_function}) is sufficiently small. If there are several methods that come to similar results, then of course the one that needs the smallest grouped portfolio (i.e. smallest $\tilde{n}$) to generate the desired cash flows is of interest. 


This thesis reviews the currently used technique for grouping policies together and introduces furthermore some new approaches on how life insurance policies can be grouped together. We will therefore highlight drawbacks and advantages with a special emphasis on the regulatory requirements of every approach discussed. Theoretical considerations as well as practical implementations and tests with real world data will provide us some information on which method an insurance company should work with in order to obtain the best grouping results.  

This thesis is structured as follows: First we give an overview on the legal framework which lays down the minimum requirements for grouping-approaches in insurance companies and introduce the two types of statistical learning, namely supervised and unsupervised. We then discuss how sensitive main characteristics of a policy are with respect to the interest rate, the age or the duration to get a better understanding on which parameters are important for grouping purposes. We therefore make a sensitivity analysis with real world data and a widely used projection tool . In the next chapter we introduce the currently used unsupervised learning algorithm k-means and derive some theoretical findings. 
\todo{Weitere Details fÃ¼r jedes Kapitel folgen.}


%--------------------------------------------------------------------- Legal Framework -----------------------------------------------------------------------
\section{Legal Framework}

Solvency II - entered into force on 1 January 2016 - is the European framework for a common insurance supervision. It is intended to achieve a harmonization of the European insurance sector and was implemented in accordance with the Lamfalussy architecture which works on a 4 level basis \cite{Lamfalussy_homepage}. The most significant elements and aims of the new regulation framework can be studied on the homepage of the financial market authority (FMA) \cite{FMA_homepage} and on the homepage of the  European Insurance and Occupational Pensions Authority (EIOPA) \cite{EIOPA_homepage}. This work is intended not to cover all aspects and aims of the new Solvency II regulation framework but focuses on the topic of data quality regarding to the actuarial function. In order to meet all the requirements imposed by Solvency II, insurance companies need to process large amounts of data within a short period. One critical aspect of these calculations is the projection horizon which however should cover the full lifetime of all obligations as stated in \cite{Time_horizon}:
\articlequote{3.83.}{}{The projection horizon used in the calculation of best estimate should
cover the full lifetime of all obligations related to existing insurance and
reinsurance contracts on the date of the valuation.}
\articlequote{3.84.}{}{The determination of the lifetime of insurance and reinsurance obligations shall be based on up-to-date and credible information and realistic assumptions about when the existing insurance and reinsurance obligations will be discharged or cancelled or expired.}
Another aspect needed to be considered is the fact that cash flow calculations need to be done for a variety of different economic scenarios which yields to an enormous computational effort. Due to the tight time schedule, insurance companies are looking for new possibilities to speed up these time consuming calculations. One approach is not to make all these calculations on a per policy level, but on a grouped level where similar policies are grouped together and represented by only a few policies. This approach raises the question of how to maintain data quality as mentioned in the level 1 directive \cite{Directive} while reducing the number of policies.

\articlequote{Article 82}{Data quality and application of approximations, including
case-by-case approaches, for technical provisions}{Member States shall ensure that insurance and reinsurance undertakings
have internal processes and procedures in place to ensure the appropriateness, completeness and accuracy of the data used in the calculation of their technical provisions...}

By publishing the level 2 regulations, supplementing the level 1 directive \cite{Directive} the European Commission is getting more specific on data quality (Article 19 in \cite{Regulations}) and also formulates concrete requirements for grouped policies \cite{Regulations}.

\articlequote{Article 35}{Homogeneous risk groups of life insurance obligations}{The cash flow projections used in the calculation of best estimates for life insurance obligations shall be made separately for each policy. Where the separate calculation for each policy would be an undue burden on the insurance or reinsurance undertaking, it may carry out the projection by grouping policies, provided that the grouping complies with all of the following requirements:
\begin{enumerate}[label=\emph{\alph*})]
\item there are no significant differences in the nature and complexity of the risks underlying the policies that belong to the same group;
\item the grouping of policies does not misrepresent the risk underlying the policies and does not misstate their expenses;
\item the grouping of policies is likely to give approximately the same results for the best estimate calculation as a calculation on a per policy basis, in particular in relation to financial guarantees and contractual options included in the policies.
\end{enumerate} }

These level 2 regulations are a reference point on what to consider when grouping policies together and they are even further specified in the level 3 guidelines issued by EIOPA\cite{Guidelines_TP}. Further details on the level 3 guidelines including feedback statements to the consultation paper (EIOPACP-14/036) and the guidelines can be obtained from \cite{Final_Report}.

%--------------------------------------------------------------------- Statistical Learning -----------------------------------------------------------------------

\section{Statistical Learning}

Statistical learning refers to a set of methods which deals with predicting outcomes based on input variables or finding patterns in data sets. In order to accomplish the task of grouping together similar policies, different approaches from statistical learning can be applied. All these methods can be classified either as supervised or unsupervised. Within the framework of supervised methods, statistical models try to predict output variables $y_i$ based on some input variables $x_i$ where the relation $y=f(x)$ is unknown. It is therefore indispensable to have input as well as output data to parameterize such a model in order to find a prediction $\hat f$ of $f$. Unsupervised methods, in contrast, are used when inputs $x_i$ but no corresponding outputs $y_i$ are available. These methods then try to find some hidden patterns within to data. The task of grouping insurance policies involves many different aspects. On the one hand we have all data needed to apply supervised methods, but on the other hand we are only interested in the patterns that can be revealed by an unsupervised method. The input variables are given by the characteristics of each policy and the corresponding output variables are determined by the projection tool. Our primary goal is not to get a good $\hat f$ because the projection tool, which stands for $f$, is already known. We are more interested in hidden patterns that can be used for grouping purposes. In a first step we will apply unsupervised methods to the data and try to group the policies based on their characteristics and their cash flows. In a further step we will try to use the additional information of $f$ to improve the grouping results if possible. 

