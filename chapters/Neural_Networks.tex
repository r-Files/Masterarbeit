%% ========================================================================
%%							NNLS
%% ========================================================================


\chapter{Neural Networks (NN)}
\label{cha:NN}

In the last chapter of this thesis, a mathematical concept is discussed that has attracted much attention in the recent past, namely neural networks. The area of artificial intelligence (AI) in which neural networks are embedded has been the subject of an intense media hype in recent years. In 2016, for example, a computer program developed by the British company Google DeepMind succeeded for the first time in defeating Lee Sedol of South Korea, considered to be the strongest Go player in the world \cite{wiki_01}. This victory of a machine against a human being is considered to be a milestone in the field of artificial intelligence \cite{LA_Times}. Further successes were also achieved in the area of real-time games at the beginning of 2019. For the first time DeepMind's program called AlphaStar was able to defeat the world's best players in StarCraft which is considered to be one of the most challenging real-time strategy games \cite{AlphaStar}. Also Gartner, a global research and advisory firm, which publishes the well-known but definitely criticisable hype cycle representations considers AI as one of the most important technologies of recent years. In the hype cyle for emerging technologies from 2017 there are 4 out of the 32 listed technologies that can be attributed to the field of AI, such as Deep Learning or Machine Learning \cite{Gartner2017}. Also in the following years 2018 and 2019 technologies that clearly belong to the AI sector were mentioned in the hype cycle 5 and 6 times respectively \cite{Gartner2018}, \cite{Gartner2019}. This trend towards new methods based on neruonal networks can also be seen in other places, such as Kaggle. With more than one million registered users, Kaggle is one of the world's largest platforms for data science competitions and attracts teams from all over the world with prize money in the millions \cite{Kaggle}. It can be observed that, besides gradient boosting, one of the most important concepts with which to win Kaggle competitions is the concept of neural networks in various forms.   

It is therefore clear to see that on the one hand the technology behind AI has enormous potential to solve problems that have so far been assumed to be solved only by humans. On the other hand, it can be assumed that this trend is not just a short-lived phenomenon, but a continuous process leading to business solutions which are based on AI-systems. Therefore, it is even more important to understand the underlying concepts of these technologies and to discuss their applicability in the insurance industry. Whether in the end a completely automated grouping algorithm based on neural networks is possible at all or can be implemented with the available resources remains open. However, the aim of this section is to provide an overview of the basic concepts and to present case studies with insurance cashflows. Since various terms and buzzwords related to artificial intelligence are often used differently in media reports, it is useful to classify some of the most often used terms in order to show their relationship and provide a general framework which is based on \cite{Allaire2018}. 

\begin{figure}
	\centering
	\includegraphics[width=0.5\textwidth]{figures/chapter_NN/framework}
	\caption{Relation of Artificial Intelligence, Machine Learning, and Deep Learning.}
	\label{fig:framework}
\end{figure}

\begin{itemize}
	\item Artificial Intelligence: Artificial Intelligence is a branch of computer science that deals with the programming of intelligent computer systems. Due to a missing definition of the term intelligence the question which types of computer programs are included is not clearly definable. In addition to machine learning and deep learning, there are many other approaches that do not include any kind of learning but are also part of AI. 
	\item Machine Learning: Machine learning arises from this question if it is possible to design a program in such a way that it can learn how to perform a specific task automatically. Given a set of input data and the corresponding results the task is to derive rules. These rules are therefore the output of the machine learning algorithm and can then be used to derive the results for new input data. What all these methods have in common is the fact that they actually try to identify statistical patterns in the data. Thus the aim is to find a meaningful representation of the given data by projections, translations, rotations, nonlinear transformations or any other method and to apply it then to new input data. 	 
	\item Deep Learning: Deep learning, as a sub field of machine learning, focuses on the progressive learning of several levels of abstraction which are increasingly meaningful representations of the data. One can think of the method as a multistage information distillation process where in every single process step a more meaningful representation of the data is obtained. The term deep in deep learning is a reference to the multiple layers which store the abstract representations of the input data in such an algorithm. The number of layers that contribute to a meaningful representation of the data is called depth. Although there is a considerable amount of learning involved, models with several hundred layers are quite common, depending on the type of problem at hand. The concept of the neural network is a reference to the field of neurobiology and the neurons that are connected in different ways in the human brain. Although neuronal networks are not models of the human brain, it is surprising what amazing results can be achieved with such a simple idea and a sufficiently large amount of data.
\end{itemize}
After a brief classification of the terms, the next section describes the basic structure and functionality of neural networks.

\section{Fundamentals of Neural Networks}

In the previous section the functionality of neuronal networks was already described in a rather abstract way as a multi-stage information distillation process. After this high level explanation, the focus is now on pointing out which individual components make up a simple neural network and how they interact. Figure (\ref{fig:NN_overview}) shows a simplified representation of the individual components required to build one of the most basic neural networks possible. Starting point for explaining the learning process of a neural network is a data set consisting of both the input and the associated output data. If the cash flows of insurance policies are to be simulated, the individual policy parameters can be used as input data and the associated cash flows as output data. These two exogenous quantities are then processed in several steps.

\begin{figure}
	\centering
	\includegraphics[width=0.65\textwidth]{figures/chapter_NN/NN_overview}
	\caption{Simplified representation of a neural network from \cite{Allaire2018}}
	\label{fig:NN_overview}
\end{figure}

\begin{enumerate}[label=Step \arabic*:]
	\item The process is initiated by providing the input data \texttt{Input X} in a suitably format to the first layer. 	
	\item This first layer then performs a data transformation based on some weights associated with that specific layer. In the first cycle those weights are randomly initialized.
	\item The transformed output from the first layer is then passed to the next layer and serves as its input. This layer also performs a data transformation based on weights which are again initialized randomly for the first cycle.
	\item After the data has passed through the last data transformation layer of the model the transformed output values are used as \texttt{Predictions Y'}.
	\item A predefined function, called the \texttt{Loss function}, measures the quality of the networkâ€™s output by comparing the \texttt{Predictions Y'} and the \texttt{True targets Y}. The result is the \texttt{loss score}, which is used as a feedback signal to adjust the \texttt{weights} associated with the layers.
	\item The task of the \texttt{Optimizer} is to take the \texttt{loss score} as an input and update the \texttt{weights} in such a way that the \texttt{loss score} is lowered.
\end{enumerate}

\begin{remark}
	The choice of the correct loss function is of great importance. If the loss function does not correctly reflect an improvement of the model, the neural net will inevitably drift in the wrong direction.
\end{remark}
\begin{remark}
	As already explained in the previous chapters, in the specific case of cash flow matching the sum of the squared residuals would be suitable as a loss score.
\end{remark}

The above steps describe a single learning cycle for a neural network. Of course, passing the input data through the net once is not sufficient to find combinations of weights that would lead to good prediction results at all. So what learning means, is to find a set of values for all weights such that the input is mapped correctly to the output for as many different input combinations as possible. This learning is achieved by grouping the input data into so-called batches and repeatedly passing those batches through the network. Associated with a batch is the batch size which defines the number of training samples sent through the neural network before the weights are adjusted by the optimizer.  
\begin{remark}
	Three different batch size approaches can be used to learn the weights in a network.
	\begin{itemize}
		\item Batch Gradient Descent: Batch size is equal to the size of the training samples.
		\item Stochastic Gradient Descent: Batch size is equal to one.
		\item Mini-Batch Gradient Descent: Batch size is bigger than one but less than the size of the training samples.
	\end{itemize} 
\end{remark}

Depending on the design of the network there can be millions of weights that need to be learned. Adjusting those millions of weights based on the feedback signal is by far the most computing intensive part of training a network. Due to the fact that the learning process is based on a large number of similar matrix operations it is possible to use highly parallelized algorithms. The availability of relatively affordable hardware that can efficiently handle such parallelizable tasks is besides the existence of large amount of data another  major reason why neural networks took off in recent years.  

\begin{remark}
	 Compared to a pure CPU system, graphics cards with their massively parallel architecture are predestined for handling such training tasks and can therefore massively reduce the time needed to train a network.
\end{remark}

After providing an brief overview of all components used in a neural network and presenting the idea of how a network learns, the next section focuses on the internals of a single layer.

\subsection{Single neuron}

The so called neurons form the basis of every neural network and are the elements which carry out the data transformations. Each layer of a neural network consists of one or more neurons which are connected differently depending on the structure of the underlying network. The task of a neuron is to take a predefined number of input values and map them to an one dimensional output. Figure (\ref{fig:simple_neuron}) shows a single neuron with all its associated components needed to perform the data transformation given by:

\begin{figure}
	\centering
	\begin{tikzpicture}[>=latex]
	\path
	(0,0)     node[circle,draw,scale=2,inner sep=2pt] (S) {$\Sigma$}
	+(90:2.5) node[circle,draw,inner sep=2.5pt] (b) {}
	node[above=1mm] {$b$}
	+(-3.5,1.5)  	node[circle, draw=black]  	(x1) 	{$x_1$}
	+(-3.5,0)    	node[circle, draw=black]  	(x2) 	{$x_2$}
	+(-3.5,-1.5) 	node[circle, draw=black]  	(x3) 	{$x_3$}
	(2,0)    		node[draw] 					(g) 	{$g$} node[above=3mm]{Activation}
	+(0:3)  		node[circle, draw=black]  	(y1) 	{$\hat{y}$};
	\draw[->] (S)--(g);
	\draw[->] (b)--(S);
	\draw[->] (g)--(y1);
	\draw[->] (x1)--(S) node[pos=.4,above]{$\omega_{1}$};
	\draw[->] (x2)--(S) node[pos=.4,above]{$\omega_{2}$};
	\draw[->] (x3)--(S) node[pos=.4,above]{$\omega_{3}$};
	\draw[black] (1,0) circle(2);
	\end{tikzpicture}

\caption{Components of a single neuron.}
\label{fig:simple_neuron}
\end{figure}

\begin{equation} \label{eq:simple_neuron}
\begin{split}
	\hat{y}		& = g\bigg( \sum_{i=1}^{3} \omega_i x_i + b \bigg) \\
				& = g(w^\top x + b) \\
				& = g(z)
\end{split}
\end{equation}
In the example shown in figure (\ref{fig:simple_neuron}) the neuron takes the three input values $x_1, x_2 \text{ and } x_3$ and transforms them in several steps into the output value $\hat{y}$. In the first step, a weighted sum is formed from the input values and the corresponding weights $w_1, w_2 \text{ and } w_3$. A bias $b$ is then added to this weighted sum and the intermediate result is called $z$. This intermediate result is then the input for the activation function $g$. Applying the activation function to $z$ gives the output value of the neuron. The purpose of the activation function can be seen as a gate. Since both the values of the inputs and the values of the weights are unrestricted, the value of $z$ can be in the interval (-$\infty$, $+\infty$). A natural way of determining whether the next neuron should be activated or not is to apply a transformation to $z$, which is done by activation function. For this reason neural networks use non-linear activation functions. Those activation functions can limit the value range and also enable the network to learn complex data structures. Some of the most commonly used activation functions in neural networks are shown in figure (\ref{fig:activation_func}): 

\begin{figure}
	\centering
	\includegraphics[width=0.75\textwidth]{figures/chapter_NN/activation_functions}
	\caption{Commonly used activation functions for neural networks.}
	\label{fig:activation_func}
\end{figure}


\begin{itemize}
	\item Sigmoid function: 
		\begin{equation}
		\begin{split}
			g: \R 	& \mapsto (0,1) \\
			g(x) 	& = \frac{1}{1+e^{-x}} 
		\end{split}
		\end{equation}
		The sigmoid function has an order of continuity of $C^{\infty}$ and is convex for all values less than 0, and it is concave for all values greater than 0. Since the output is always in the range from 0 to 1, one of the use cases of the sigmoid function is to model probabilities. Applying the sigmoid function to strongly negative values results in values close to zero. This means that the next neuron is only activated if the linear transformation has given a value $z$ that is not too negative. 
	\item Hyperbolic tangent function: 
		\begin{equation}
		\begin{split}
			g: \R 	& \mapsto (-1,1) \\
			g(x) 	& = \frac{e^x - e^{-x}}{e^x + e^{-x}} 
		\end{split}
		\end{equation}
		The hyperbolic tangent function is very similar to the sigmoid function and, just like the sigmoid function, also has an order of continuity of $C^{\infty}$. Since the range of values lies between -1 and 1, the possibilities of using this activation function as a gate are rather limited compared to the sigmoid function. Possible application scenarios can be found in the area of classification problems where all outputs below zero belong to class $A$ and all outputs greater than zero belong to class $B$. 
		
	\item ReLu function: 
		\begin{equation}
		\begin{split}
			g: \R 	& \mapsto [0,+ \infty) \\
			g(x) 	& = max(0,x) 
		\end{split}
		\end{equation}
		One of the most successful and widely-used activation functions is the Rectified Linear Unit (ReLU). All values greater than zero activate the next neuron and all negative input values result in the next neuron not being activated by setting the output to zero. Because of the maximization the ReLu-function has only an order of continuity of $C^0$. Although it is non-differentiable, this function is popular because of it's simplicity and reliability as a gate function. \cite{searchingActivation}, \cite{nair2010rectified}
			
	\item Softplus function: 
		\begin{equation}
		\begin{split}
			g: \R 	& \mapsto (0,\infty) \\
			g(x) 	& = ln(1 + e^x) 
		\end{split}
		\end{equation}	
		 Since ReLu is non-differentiable at zero a smooth version of it, which also has an order of continuity of $C^{\infty}$, is given by the softplus function. It can be used as a replacement for the ReLu function. As various analyses have shown the performance between of softplus as activation function is comparable to that of the Relu function. \cite{searchingActivation}
\end{itemize}

In the example given in figure (\ref{fig:simple_neuron}), the output value of the neuron is already the prediction $\hat{y}$, but in almost every case the output of one neuron serves as an input for another neuron. The next section is therefore dedicated to the interaction of multiple neurons in multiple layers and introduces a general notation that allows to describe the mathematical model of a neural network including an optimization algorithm.


